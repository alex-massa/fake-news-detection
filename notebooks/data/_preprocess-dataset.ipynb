{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cbce2-938c-46dd-a2c5-496fb5dd212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./../utils/_logger.ipynb\n",
    "%run ./../utils/_preprocess-utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ed2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606949a-285c-4000-a5e0-c2a25f11adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES_TO_LOAD = 300000\n",
    "N_MAX_SAMPLES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11131b13-9ee3-4598-8091-17be17c0b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 300\n",
    "MIN_UNIQUE = 50\n",
    "MIN_OCCURENCES = 5\n",
    "NO_ABOVE = .5\n",
    "DOCUMENT_FILTERS = (deaccent, lower_to_unicode, strip_tags, strip_punctuation, \n",
    "                    strip_non_alphanum, split_alphanum, strip_numeric, strip_short, \n",
    "                    remove_stopwords, strip_multiple_whitespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94316d-b097-43a3-ab8b-64fe20d0e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SOURCE_DATASET_PATH, nrows=N_SAMPLES_TO_LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eab75d-e922-47d7-8e42-a76b2a138a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty and duplicate documents\n",
    "df.dropna(subset=[doc_col], inplace=True)\n",
    "df.drop_duplicates(subset=[doc_col], keep='first', inplace=True, ignore_index=True)\n",
    "df.drop_duplicates(subset=[title_col], keep='first', inplace=True, ignore_index=True)\n",
    "df.drop_duplicates(subset=[url_col], keep='first', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab49c6-fbd5-4fdf-958c-fe855674a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[proc_doc_col] = df[doc_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aaa521",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Preprocessing corpus...\")\n",
    "df[proc_doc_col] = [apply_filters(doc, filters=DOCUMENT_FILTERS) for doc in tqdm(df[proc_doc_col], disable=SILENT)]\n",
    "\n",
    "logger.info(\"Replacing special characters...\")\n",
    "df[proc_doc_col] = [sub_pattern(doc, pattern=SUB_PATTERN) for doc in tqdm(df[proc_doc_col], disable=SILENT)]\n",
    "\n",
    "logger.info(\"Removing unprintable characters...\")\n",
    "df[proc_doc_col] = [remove_unprintable(doc) for doc in tqdm(df[proc_doc_col], disable=SILENT)]\n",
    "\n",
    "logger.info(\"Tokenizing corpus...\")\n",
    "df[proc_doc_col] = [[t for t in tokenize(doc)] for doc in tqdm(df[proc_doc_col], disable=SILENT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b76c9-555d-4c70-b2e6-cbfc74ade293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminarily drop documents deemed too short and those that don't contain enough unique tokens\n",
    "if MIN_LENGTH is not None:\n",
    "    df.drop([i for i, row in df.iterrows() if len(row[proc_doc_col]) < MIN_LENGTH], inplace=True)\n",
    "if MIN_UNIQUE is not None:\n",
    "    df.drop([i for i, row in df.iterrows() if len(set(row[proc_doc_col])) < MIN_UNIQUE], inplace=True)\n",
    "\n",
    "# make auxiliary dictionary from processed corpus\n",
    "dictionary = Dictionary(df[proc_doc_col])\n",
    "# filter tokens that appear in few documents from dictionary\n",
    "dictionary.filter_extremes(no_below=MIN_OCCURENCES, no_above=NO_ABOVE, keep_n=None)\n",
    "\n",
    "logger.info(\"Removing unfrequent tokens in corpus...\")\n",
    "df[proc_doc_col] = [[t for t in doc if t in dictionary.token2id] for doc in tqdm(df[proc_doc_col], disable=SILENT)]\n",
    "\n",
    "# drop documents that contain few tokens that remained in the dictionary\n",
    "if MIN_LENGTH is not None:\n",
    "    df.drop([i for i, row in df.iterrows() if len([t for t in row[proc_doc_col] if t in dictionary.token2id]) < MIN_LENGTH], inplace=True)\n",
    "# drop documents that contain few unique tokens that remained in the dictionary\n",
    "if MIN_UNIQUE is not None:\n",
    "    df.drop([i for i, row in df.iterrows() if len({t for t in row[proc_doc_col] if t in dictionary.token2id}) < MIN_UNIQUE], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Subsampling the dataset...\")\n",
    "\n",
    "n_samples = min(len(df), N_MAX_SAMPLES)\n",
    "\n",
    "neg_label, pos_label = False, True\n",
    "neg_df, pos_df = df.loc[df[label_col] == neg_label], df.loc[df[label_col] == pos_label]\n",
    "\n",
    "n_neg = min(len(neg_df), int(n_samples * .5))\n",
    "n_pos = min(len(pos_df), int(n_samples * .5))\n",
    "\n",
    "neg_df, pos_df = neg_df.sample(n=n_neg, random_state=RANDOM_SEED), \\\n",
    "                 pos_df.sample(n=n_pos, random_state=RANDOM_SEED)\n",
    "df = pd.concat([neg_df, pos_df]).sample(frac=1, ignore_index=True, random_state=RANDOM_SEED)\n",
    "\n",
    "logger.info(\"Dataset subsampled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f20f19-f266-4661-bc14-43bee05a28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index to account for removed entries\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary from processed and subsampled corpus\n",
    "dictionary = Dictionary(df[proc_doc_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70717bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving preprocessed dataset to disk...\")\n",
    "Path(PROC_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "df.to_pickle(PROC_DATASET_PATH), dictionary.save(DICTIONARY_PATH)\n",
    "logger.info(\"Preprocessed dataset saved to disk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

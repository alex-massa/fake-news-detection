{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ed2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, doc_col='doc', proc_doc_col='proc_doc', label_col='label', filters=None,\n",
    "                    drop_below_len=None, drop_below_unique=None, drop_below_occurences=None, \n",
    "                    inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy(deep=True)\n",
    "    \n",
    "    # drop empty and duplicate documents\n",
    "    df.dropna(subset=[doc_col], inplace=True)\n",
    "    df.drop_duplicates(subset=[doc_col], keep='first', inplace=True)\n",
    "    # preprocess documents using several filters, turning each document in a list of tokens\n",
    "    df[proc_doc_col] = [preprocess_string(doc, filters=filters) for doc in df[doc_col]]\n",
    "    \n",
    "    # preliminarily drop documents deemed too short and those that don't contain enough unique words\n",
    "    if drop_below_len is not None:\n",
    "        df.drop([i for i, row in df.iterrows() if len(row[proc_doc_col]) < drop_below_len], inplace=True)\n",
    "    if drop_below_unique is not None:\n",
    "        df.drop([i for i, row in df.iterrows() if len(set(row[proc_doc_col])) < drop_below_unique], inplace=True)\n",
    "    \n",
    "    # make auxiliary dictionary from processed corpus\n",
    "    dictionary = Dictionary(df[proc_doc_col])\n",
    "    # filter tokens that appear in few documents from dictionary\n",
    "    if drop_below_occurences is not None:\n",
    "        dictionary.filter_extremes(no_below=drop_below_occurences)\n",
    "    # remove from processed corpus words that were removed from dictionary\n",
    "    df[proc_doc_col] = [[dictionary[word_idx] for word_idx\n",
    "                         in dictionary.doc2idx(doc) if word_idx != -1]\n",
    "                         for doc in df[proc_doc_col]]\n",
    "    \n",
    "    # drop documents that contain few words that remained in the dictionary\n",
    "    if drop_below_len is not None:\n",
    "        df.drop([i for i, row in df.iterrows() \n",
    "                 if len([word_idx for word_idx in dictionary.doc2idx(row[proc_doc_col]) \n",
    "                         if word_idx != -1]) < drop_below_len], inplace=True)\n",
    "    # drop documents that contain few unique words that remained in the dictionary\n",
    "    if drop_below_unique is not None:\n",
    "        df.drop([i for i, row in df.iterrows() \n",
    "                 if len({word_idx for word_idx in dictionary.doc2idx(row[proc_doc_col]) \n",
    "                         if word_idx != -1}) < drop_below_unique], inplace=True)\n",
    "    \n",
    "    # reset index to account for removed entries\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "546ab1ed",
   "metadata": {},
   "source": [
    "def subsample_data(df, positive_frac=None, max_samples=None,\n",
    "                   label_col='label', label_vals=(False, True),\n",
    "                   random_seed=None):\n",
    "    n_samples = min(len(df), max_samples) if max_samples is not None else len(df)\n",
    "    if positive_frac is None:\n",
    "        return df.sample(n=n_samples, ignore_index=True, random_state=random_seed)\n",
    "\n",
    "    neg_label, pos_label = label_vals\n",
    "    neg_df, pos_df = df.loc[df[label_col] == neg_label], df.loc[df[label_col] == pos_label]\n",
    "\n",
    "    n_neg = int(n_samples * (1.0 - positive_frac))\n",
    "    n_pos = int(n_samples * positive_frac)\n",
    "    if n_neg > len(neg_df) and not n_pos > len(pos_df):\n",
    "        n_neg = len(neg_df)\n",
    "        n_pos = (n_neg / (1.0 - positive_frac)) * positive_frac\n",
    "    elif not n_neg > len(neg_df) and n_pos > len(pos_df):\n",
    "        n_pos = len(pos_df)\n",
    "        n_neg = (n_pos / positive_frac) * (1.0 - positive_frac)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    neg_df, pos_df = neg_df.sample(n=n_neg, random_state=random_seed), \\\n",
    "                     pos_df.sample(n=n_pos, random_state=random_seed)\n",
    "    return pd.concat([neg_df, pos_df]).sample(frac=1, ignore_index=True, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_data(df, positive_frac=None, max_samples=None,\n",
    "                   label_col='label', label_vals=(False, True),\n",
    "                   random_seed=None):\n",
    "    n_samples = min(len(df), max_samples) if max_samples is not None else len(df)\n",
    "    if positive_frac is None:\n",
    "        return df.sample(n=n_samples, ignore_index=True, random_state=random_seed)\n",
    "\n",
    "    neg_label, pos_label = label_vals\n",
    "    neg_df, pos_df = df.loc[df[label_col] == neg_label], df.loc[df[label_col] == pos_label]\n",
    "\n",
    "    n_neg = min(len(neg_df), int(n_samples * (1.0 - positive_frac)))\n",
    "    n_pos = min(len(pos_df), int(n_samples * positive_frac))\n",
    "\n",
    "    neg_df, pos_df = neg_df.sample(n=n_neg, random_state=random_seed), \\\n",
    "                     pos_df.sample(n=n_pos, random_state=random_seed)\n",
    "    return pd.concat([neg_df, pos_df]).sample(frac=1, ignore_index=True, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dea4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading dataset from disk...\")\n",
    "df = pd.read_csv(DATASET_PATH, nrows=N_SAMPLES_TO_LOAD)\n",
    "logger.info(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aaa521",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Preprocessing the dataset...\")\n",
    "df = preprocess_data(df, doc_col=doc_col, proc_doc_col=proc_doc_col, label_col=label_col, \n",
    "                     drop_below_len=MIN_LENGTH, drop_below_unique=MIN_UNIQUE, \n",
    "                     drop_below_occurences=MIN_OCCURENCES, \n",
    "                     filters=DOCUMENT_FILTERS, inplace=True)\n",
    "logger.info(\"Dataset preprocessed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Subsampling the dataset...\")\n",
    "df = subsample_data(df, positive_frac=POS_FRAC, max_samples=N_MAX_SAMPLES,\n",
    "                    label_col=label_col, label_vals=(False, True),\n",
    "                    random_seed=RANDOM_SEED)\n",
    "logger.info(\"Dataset subsampled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952bd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary from processed corpus\n",
    "dictionary = Dictionary([['<PAD>']] + df[proc_doc_col].values.tolist())\n",
    "dictionary.cfs[0] = dictionary.dfs[0] = 0\n",
    "dictionary.num_docs -= 1\n",
    "dictionary.num_pos -= 1\n",
    "dictionary.num_nnz -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70717bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving preprocessed dataset to disk...\")\n",
    "Path(PROC_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "df.to_pickle(PROC_DATASET_PATH), dictionary.save(DICTIONARY_PATH)\n",
    "logger.info(\"Preprocessed dataset saved to disk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import wraps\n",
    "from enum import Enum\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_generator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.is_generator:\n",
    "            return func(self, *args, **kwargs)\n",
    "        for values in func(self, *args, **kwargs):\n",
    "            pass\n",
    "        return values\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Enum):\n",
    "    RELU = lambda x: (x > 0) * x, \\\n",
    "           lambda x: x > 0\n",
    "\n",
    "    SIGMOID = lambda x: 1 / (1 + np.exp(-x)), \\\n",
    "              lambda x: x * (1 - x)\n",
    "\n",
    "    TANH = lambda x: np.tanh(x), \\\n",
    "           lambda x: 1 - (x ** 2),\n",
    "    \n",
    "    SOFTMAX = lambda x: (ex := np.exp(x)) / np.sum(ex, axis=1, keepdims=True), \\\n",
    "              lambda x: (_ for _ in ()).throw(Exception(\"activation function softmax only works on output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuralNetwork:\n",
    "\n",
    "    def __init__(self, hid_size, epochs=3, alpha=1e-3, batch_size=32, skip_remaining=True,\n",
    "                 hid_activation=None, out_activation=None, dropout=1,\n",
    "                 as_probs=False, is_generator=False, random_seed=None):\n",
    "        self.hid_size = hid_size\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_remaining = skip_remaining\n",
    "        self.hid_activation = hid_activation\n",
    "        self.out_activation = out_activation\n",
    "        self.dropout = dropout\n",
    "        self.as_probs = as_probs\n",
    "        self.is_generator = is_generator\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if self.hid_activation is not None:\n",
    "            self.__hid_activation_fun, self.__hid_activation_deriv = self.hid_activation.value\n",
    "        if self.out_activation is not None:\n",
    "            self.__out_activation_fun, _ = self.out_activation.value\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            self.__to_probs, _ = Activation.SOFTMAX.value\n",
    "\n",
    "        self.__rng = np.random.default_rng(seed=self.random_seed)\n",
    "    \n",
    "    @conditional_generator\n",
    "    def fit(self, train_samples, train_labels):\n",
    "        assert(len(train_samples) == len(train_labels))\n",
    "\n",
    "        # initialize coefficients with values between -0.1 and 0.1\n",
    "        self.coeffs_in_to_hid = 0.2 * self.__rng.random((len(train_samples.T), self.hid_size)) - 0.1\n",
    "        self.coeffs_hid_to_out = 0.2 * self.__rng.random((self.hid_size, len(train_labels.T))) - 0.1\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(0, len(train_samples), self.batch_size):\n",
    "                samples = train_samples[i:i + self.batch_size]\n",
    "                labels = train_labels[i:i + self.batch_size]\n",
    "                batch_size = len(samples)\n",
    "                if batch_size < self.batch_size and self.skip_remaining:\n",
    "                    continue\n",
    "\n",
    "                dropout_mask = self.__rng.choice((0, 1), size=(batch_size, self.hid_size), \n",
    "                                                 p=(1 - self.dropout, self.dropout))\n",
    "\n",
    "                layers_in = samples\n",
    "                layers_hid = layers_in.dot(self.coeffs_in_to_hid)\n",
    "                if self.hid_activation is not None:\n",
    "                    layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "                layers_hid *= dropout_mask * (1 / self.dropout)\n",
    "                layers_out = layers_hid.dot(self.coeffs_hid_to_out)\n",
    "                if self.out_activation is not None:\n",
    "                    layers_out = self.__out_activation_fun(layers_out)\n",
    "\n",
    "                deltas_out = labels - layers_out\n",
    "                deltas_hid = deltas_out.dot(self.coeffs_hid_to_out.T)\n",
    "                if self.hid_activation is not None:\n",
    "                    deltas_hid *= self.__hid_activation_deriv(layers_hid)\n",
    "                deltas_hid *= dropout_mask\n",
    "\n",
    "                self.coeffs_hid_to_out += layers_hid.T.dot(deltas_out) * self.alpha\n",
    "                self.coeffs_in_to_hid += layers_in.T.dot(deltas_hid) * self.alpha\n",
    "            yield\n",
    "\n",
    "    def predict(self, samples):\n",
    "        assert(len(samples.T) == len(self.coeffs_in_to_hid))\n",
    "\n",
    "        # predictions for test samples are made in one batch\n",
    "        layers_in = samples\n",
    "        layers_hid = layers_in.dot(self.coeffs_in_to_hid)\n",
    "        if self.hid_activation is not None:\n",
    "            layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "        layers_out = layers_hid.dot(self.coeffs_hid_to_out)\n",
    "        if self.out_activation is not None:\n",
    "            layers_out = self.__out_activation_fun(layers_out)\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            layers_out = self.__to_probs(layers_out)\n",
    "\n",
    "        return layers_out\n",
    "\n",
    "    def evaluate(self, preds, labels):\n",
    "        assert(len(preds) == len(labels))\n",
    "        assert(len(preds.T) == len(labels.T) == len(self.coeffs_hid_to_out.T))\n",
    "        errors = ((labels - preds) ** 2).sum(axis=0)\n",
    "        loss = sum(errors) / len(preds)\n",
    "        n_correct = sum([np.argmax(pred) == np.argmax(label)\n",
    "                        for pred, label in zip(preds, labels)])\n",
    "        accuracy = n_correct / len(preds)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSparseNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, dict_size, hid_size, epochs=3, alpha=1e-3,\n",
    "                 hid_activation=None, out_activation=None, dropout=1, as_probs=False,\n",
    "                 factor_words_freq=True, is_generator=False, random_seed=None):\n",
    "        self.dict_size = dict_size\n",
    "        self.hid_size = hid_size\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.hid_activation = hid_activation\n",
    "        self.out_activation = out_activation\n",
    "        self.dropout = dropout\n",
    "        self.as_probs = as_probs\n",
    "        self.factor_words_freq = factor_words_freq\n",
    "        self.is_generator = is_generator\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if self.hid_activation is not None:\n",
    "            self.__hid_activation_fun, self.__hid_activation_deriv = self.hid_activation.value\n",
    "        if self.out_activation is not None:\n",
    "            self.__out_activation_fun, _ = self.out_activation.value\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            self.__to_probs, _ = Activation.SOFTMAX.value\n",
    "\n",
    "        self.__rng = np.random.default_rng(seed=self.random_seed)\n",
    "\n",
    "    @conditional_generator\n",
    "    def fit(self, train_samples, train_labels):\n",
    "        assert(len(train_samples) == len(train_labels))\n",
    "\n",
    "        # initialize coefficients using values between -0.1 and 0.1\n",
    "        self.coeffs_in_to_hid = 0.2 * self.__rng.random((self.dict_size, self.hid_size)) - 0.1\n",
    "        self.coeffs_hid_to_out = 0.2 * self.__rng.random((self.hid_size, len(train_labels.T))) - 0.1\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for sample, label in zip(train_samples, train_labels):\n",
    "                dropout_mask = self.__rng.choice((0, 1), size=self.hid_size, \n",
    "                                                 p=(1 - self.dropout, self.dropout))\n",
    "\n",
    "                layer_in = sample\n",
    "                if self.factor_words_freq:\n",
    "                    layer_hid = layer_in.T[1].dot(self.coeffs_in_to_hid[layer_in.T[0]])\n",
    "                else:\n",
    "                    layer_hid = self.coeffs_in_to_hid[layer_in.T[0]].sum(axis=0)\n",
    "                if self.hid_activation is not None:\n",
    "                    layer_hid = self.__hid_activation_fun(layer_hid)\n",
    "                layer_hid *= dropout_mask * (1 / self.dropout)\n",
    "                layer_out = layer_hid.dot(self.coeffs_hid_to_out)\n",
    "                if self.out_activation is not None:\n",
    "                    layer_out = self.__out_activation_fun(layer_out)\n",
    "\n",
    "                deltas_out = label - layer_out\n",
    "                deltas_hid = deltas_out.dot(self.coeffs_hid_to_out.T)\n",
    "                if self.hid_activation is not None:\n",
    "                    deltas_hid *= self.__hid_activation_deriv(layer_hid)\n",
    "                deltas_hid *= dropout_mask\n",
    "\n",
    "                self.coeffs_hid_to_out += np.outer(layer_hid, deltas_out) * self.alpha\n",
    "                self.coeffs_in_to_hid[layer_in.T[0]] += deltas_hid * self.alpha\n",
    "            yield\n",
    "\n",
    "    def predict(self, samples):\n",
    "        preds = []\n",
    "        for sample in samples:\n",
    "            layer_in = sample\n",
    "            if self.factor_words_freq:\n",
    "                layer_hid = layer_in.T[1].dot(self.coeffs_in_to_hid[layer_in.T[0]])\n",
    "            else:\n",
    "                layer_hid = self.coeffs_in_to_hid[layer_in.T[0]].sum(axis=0)\n",
    "            if self.hid_activation is not None:\n",
    "                layer_hid = self.__hid_activation_fun(layer_hid)\n",
    "            layer_out = layer_hid.dot(self.coeffs_hid_to_out)\n",
    "            if self.out_activation is not None:\n",
    "                layer_out = self.__out_activation_fun(layer_out)\n",
    "\n",
    "            preds.append(layer_out)\n",
    "        preds = np.array(preds)\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            preds = self.__to_probs(preds)\n",
    "        return preds\n",
    "\n",
    "    def evaluate(self, preds, labels):\n",
    "        assert(len(preds) == len(labels))\n",
    "        assert(len(preds.T) == len(labels.T) == len(self.coeffs_hid_to_out.T))\n",
    "        errors = ((labels - preds) ** 2).sum(axis=0)\n",
    "        loss = sum(errors) / len(preds)\n",
    "        n_correct = sum([np.argmax(pred) == np.argmax(label)\n",
    "                        for pred, label in zip(preds, labels)])\n",
    "        accuracy = n_correct / len(preds)\n",
    "        return loss, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import wraps\n",
    "from enum import Enum\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_generator(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        if self.yield_results:\n",
    "            return func(self, *args, **kwargs)\n",
    "        for values in func(self, *args, **kwargs):\n",
    "            pass\n",
    "        return values\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Enum):\n",
    "    RELU = lambda x: (x > 0) * x, \\\n",
    "           lambda x: x > 0\n",
    "\n",
    "    SIGMOID = lambda x: 1 / (1 + np.exp(-x)), \\\n",
    "              lambda x: x * (1 - x)\n",
    "\n",
    "    TANH = lambda x: np.tanh(x), \\\n",
    "           lambda x: 1 - (x ** 2),\n",
    "    \n",
    "    SOFTMAX = lambda x: (ex := np.exp(x)) / np.sum(ex, axis=1, keepdims=True), \\\n",
    "              lambda x: (_ for _ in ()).throw(Exception(\"activation function softmax is only for outer layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b369fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuralNetwork:\n",
    "\n",
    "    def __init__(self, hid_size, epochs=3, alpha=1e-3, batch_size=32, skip_remaining=True,\n",
    "                 hid_activation=None, out_activation=None, dropout=False, as_probs=False,\n",
    "                 yield_results=False, silent=True, random_seed=None):\n",
    "        self.hid_size = hid_size\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_remaining = skip_remaining\n",
    "        self.hid_activation = hid_activation\n",
    "        self.out_activation = out_activation\n",
    "        self.dropout = dropout\n",
    "        self.as_probs = as_probs\n",
    "        self.yield_results = yield_results\n",
    "        self.silent = silent\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if self.hid_activation is not None:\n",
    "            self.__hid_activation_fun, self.__hid_activation_deriv = self.hid_activation.value\n",
    "        if self.out_activation is not None:\n",
    "            self.__out_activation_fun, _ = self.out_activation.value\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            self.__to_probs, _ = Activation.SOFTMAX.value\n",
    "\n",
    "        self.__rng = np.random.default_rng(seed=self.random_seed)\n",
    "    \n",
    "    @conditional_generator\n",
    "    def fit(self, train_samples, train_labels):\n",
    "        assert(len(train_samples) == len(train_labels))\n",
    "        \n",
    "        # initialize coefficients with values between -0.1 and 0.1\n",
    "        self.coeffs_in_to_hid = 0.2 * self.__rng.random((len(train_samples.T), self.hid_size)) - 0.1\n",
    "        self.coeffs_hid_to_out = 0.2 * self.__rng.random((self.hid_size, len(train_labels.T))) - 0.1\n",
    "\n",
    "        for _ in tqdm(range(self.epochs), disable=self.silent):\n",
    "            preds_iter, errors_iter = [], []\n",
    "            for i in range(0, len(train_samples), self.batch_size):\n",
    "                samples = train_samples[i:i + self.batch_size]\n",
    "                labels = train_labels[i:i + self.batch_size]\n",
    "                batch_size = len(samples)\n",
    "                if batch_size < self.batch_size and self.skip_remaining:\n",
    "                    continue\n",
    "\n",
    "                # used to disable appproximately half the weight\n",
    "                if self.dropout:\n",
    "                    dropout_mask = self.__rng.integers(2, size=(batch_size, self.hid_size))\n",
    "\n",
    "                layers_in = samples\n",
    "                layers_hid = layers_in.dot(self.coeffs_in_to_hid)\n",
    "                if self.hid_activation is not None:\n",
    "                    layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "                if self.dropout:\n",
    "                    layers_hid *= dropout_mask * 2\n",
    "                layers_out = layers_hid.dot(self.coeffs_hid_to_out)\n",
    "                if self.out_activation is not None:\n",
    "                    layers_out = self.__out_activation_fun(layers_out)\n",
    "\n",
    "                preds = layers_out\n",
    "                errors = ((labels - preds) ** 2).sum(axis=0)\n",
    "                preds_iter.extend(preds), errors_iter.extend(errors)\n",
    "\n",
    "                deltas_out = labels - preds\n",
    "                deltas_hid = deltas_out.dot(self.coeffs_hid_to_out.T)\n",
    "                if self.hid_activation is not None:\n",
    "                    deltas_hid *= self.__hid_activation_deriv(layers_hid)\n",
    "                if self.dropout:\n",
    "                    deltas_hid *= dropout_mask\n",
    "\n",
    "                self.coeffs_hid_to_out += layers_hid.T.dot(deltas_out) * self.alpha\n",
    "                self.coeffs_in_to_hid += layers_in.T.dot(deltas_hid) * self.alpha\n",
    "\n",
    "            preds_iter = np.array(preds_iter)\n",
    "            errors_iter = np.array(errors_iter)\n",
    "            if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "                preds_iter = self.__to_probs(preds_iter)\n",
    "            yield (preds_iter, errors_iter)\n",
    "\n",
    "    def predict(self, samples, labels=None):\n",
    "        assert(len(samples.T) == len(self.coeffs_in_to_hid))\n",
    "        if labels is not None:\n",
    "            assert(len(samples) == len(labels))\n",
    "            assert(len(labels.T) == len(self.coeffs_hid_to_out.T))\n",
    "\n",
    "        # predictions for test samples are made in one batch\n",
    "        layers_in = samples\n",
    "        layers_hid = layers_in.dot(self.coeffs_in_to_hid)\n",
    "        if self.hid_activation is not None:\n",
    "            layers_hid = self.__hid_activation_fun(layers_hid)\n",
    "        layers_out = layers_hid.dot(self.coeffs_hid_to_out)\n",
    "        if self.out_activation is not None:\n",
    "            layers_out = self.__out_activation_fun(layers_out)\n",
    "        \n",
    "        preds = layers_out\n",
    "        if labels is not None:\n",
    "            errors = ((labels - preds) ** 2).sum(axis=0)\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            preds = self.__to_probs(preds)\n",
    "        if labels is not None:\n",
    "            return (preds, errors)\n",
    "        return preds\n",
    "    \n",
    "    @conditional_generator\n",
    "    def evaluate(self, train_samples, test_samples, train_labels, test_labels):\n",
    "        if self.yield_results:\n",
    "            for (train_preds, train_errors) in self.fit(train_samples, train_labels):\n",
    "                (test_preds, test_errors) = self.predict(test_samples, test_labels)\n",
    "                yield (train_preds, train_errors), (test_preds, test_errors)\n",
    "        else:\n",
    "            (train_preds, train_errors) = self.fit(train_samples, train_labels)\n",
    "            (test_preds, test_errors) = self.predict(test_samples, test_labels)\n",
    "            yield (train_preds, train_errors), (test_preds, test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSparseNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, dict_size, hid_size, epochs=3, alpha=1e-3,\n",
    "                 hid_activation=None, out_activation=None, dropout=False, as_probs=False,\n",
    "                 factor_words_freq=True, yield_results=False, silent=True, random_seed=None):\n",
    "        self.dict_size = dict_size\n",
    "        self.hid_size = hid_size\n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.hid_activation = hid_activation\n",
    "        self.out_activation = out_activation\n",
    "        self.dropout = dropout\n",
    "        self.as_probs = as_probs\n",
    "        self.factor_words_freq = factor_words_freq\n",
    "        self.yield_results = yield_results\n",
    "        self.silent = silent\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        if self.hid_activation is not None:\n",
    "            self.__hid_activation_fun, self.__hid_activation_deriv = self.hid_activation.value\n",
    "        if self.out_activation is not None:\n",
    "            self.__out_activation_fun, _ = self.out_activation.value\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            self.__to_probs, _ = Activation.SOFTMAX.value\n",
    "\n",
    "        self.__rng = np.random.default_rng(seed=self.random_seed)\n",
    "\n",
    "    @conditional_generator\n",
    "    def fit(self, train_samples, train_labels):\n",
    "        assert(len(train_samples) == len(train_labels))\n",
    "        \n",
    "        # initialize coefficients using values between -0.1 and 0.1\n",
    "        self.coeffs_in_to_hid = 0.2 * self.__rng.random((self.dict_size, self.hid_size)) - 0.1\n",
    "        self.coeffs_hid_to_out = 0.2 * self.__rng.random((self.hid_size, len(train_labels.T))) - 0.1\n",
    "        \n",
    "        for _ in tqdm(range(self.epochs), disable=self.silent):\n",
    "            preds_iter, errors_iter = [], []\n",
    "            for sample, label in zip(train_samples, train_labels):\n",
    "                if self.dropout:\n",
    "                    dropout_mask = self.__rng.integers(2, size=self.hid_size)\n",
    "                \n",
    "                layer_in = sample\n",
    "                if self.factor_words_freq:\n",
    "                    layer_hid = layer_in.T[1].dot(self.coeffs_in_to_hid[layer_in.T[0]])\n",
    "                else:\n",
    "                    layer_hid = self.coeffs_in_to_hid[layer_in.T[0]].sum(axis=0)\n",
    "                if self.hid_activation is not None:\n",
    "                    layer_hid = self.__hid_activation_fun(layer_hid)\n",
    "                if self.dropout:\n",
    "                    # half the weights are disabled, so the result is doubled to make up for it\n",
    "                    layer_hid *= dropout_mask * 2\n",
    "                layer_out = layer_hid.dot(self.coeffs_hid_to_out)\n",
    "                if self.out_activation is not None:\n",
    "                    layer_out = self.__out_activation_fun(layer_out)\n",
    "\n",
    "                pred = layer_out\n",
    "                error = ((label - pred) ** 2).sum(axis=0)\n",
    "                preds_iter.append(pred), errors_iter.append(error)\n",
    "\n",
    "                deltas_out = label - pred\n",
    "                deltas_hid = deltas_out.dot(self.coeffs_hid_to_out.T)\n",
    "                if self.hid_activation is not None:\n",
    "                    deltas_hid *= self.__hid_activation_deriv(layer_hid)\n",
    "                if self.dropout:\n",
    "                    deltas_hid *= dropout_mask\n",
    "\n",
    "                self.coeffs_hid_to_out += np.outer(layer_hid, deltas_out) * self.alpha\n",
    "                self.coeffs_in_to_hid[layer_in.T[0]] += deltas_hid * self.alpha\n",
    "\n",
    "            preds_iter = np.array(preds_iter)\n",
    "            errors_iter = np.array(errors_iter)\n",
    "            if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "                preds_iter = self.__to_probs(preds_iter)\n",
    "            yield (preds_iter, errors_iter)\n",
    "\n",
    "    def predict(self, samples, labels=None):\n",
    "        if labels is not None:\n",
    "            assert(len(samples) == len(labels))\n",
    "            assert(len(labels.T) == len(self.coeffs_hid_to_out.T))\n",
    "        preds, errors = [], []\n",
    "        for sample, label in zip(samples, labels):\n",
    "            layer_in = sample\n",
    "            if self.factor_words_freq:\n",
    "                layer_hid = layer_in.T[1].dot(self.coeffs_in_to_hid[layer_in.T[0]])\n",
    "            else:\n",
    "                layer_hid = self.coeffs_in_to_hid[layer_in.T[0]].sum(axis=0)\n",
    "            if self.hid_activation is not None:\n",
    "                layer_hid = self.__hid_activation_fun(layer_hid)\n",
    "            layer_out = layer_hid.dot(self.coeffs_hid_to_out)\n",
    "            if self.out_activation is not None:\n",
    "                layer_out = self.__out_activation_fun(layer_out)\n",
    "            \n",
    "            pred = layer_out\n",
    "            preds.append(pred)\n",
    "            if labels is not None:\n",
    "                error = ((label - pred) ** 2).sum(axis=0)\n",
    "                errors.append(error)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        if labels is not None:\n",
    "            errors = np.array(errors)\n",
    "        if self.as_probs and self.out_activation is not Activation.SOFTMAX:\n",
    "            preds = self.__to_probs(preds)\n",
    "        if labels is not None:\n",
    "            return (preds, errors)\n",
    "        return preds\n",
    "\n",
    "    @conditional_generator\n",
    "    def evaluate(self, train_samples, test_samples, train_labels, test_labels):\n",
    "        if self.yield_results:\n",
    "            for (train_preds, train_errors) in self.fit(train_samples, train_labels):\n",
    "                (test_preds, test_errors) = self.predict(test_samples, test_labels)\n",
    "                yield (train_preds, train_errors), (test_preds, test_errors)\n",
    "        else:\n",
    "            (train_preds, train_errors) = self.fit(train_samples, train_labels)\n",
    "            (test_preds, test_errors) = self.predict(test_samples, test_labels)\n",
    "            yield (train_preds, train_errors), (test_preds, test_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
